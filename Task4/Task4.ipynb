{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Напишите программу, которая находит самое длинное слово."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/05 18:36:39 WARN Utils: Your hostname, Yoga14 resolves to a loopback address: 127.0.1.1; using 192.168.0.113 instead (on interface wlp2s0)\n",
      "23/06/05 18:36:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/05 18:36:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самое длинное слово: Галогенарены-Нафталин-Антрацен-Фенантрен-Бензпирен-Коронен-Азулен-Бифенил-Ионол.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()\n",
    "text_file = sc.textFile(\"/home/daniel/wiki.txt\")\n",
    "\n",
    "\n",
    "def is_valid_word(word):\n",
    "    if word.__contains__(\"http\"):\n",
    "        return False\n",
    "    if any(char.isdigit() for char in word):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "longest_word = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "                        .filter(is_valid_word) \\\n",
    "                        .reduce(lambda x, y: x if len(x) > len(y) else y)\n",
    "\n",
    "print(\"Самое длинное слово:\", longest_word)\n",
    "sc.stop()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Напишите программу, которая находит среднюю длину слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя длина слова: 6.433347173833233\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "text_file = sc.textFile(\"/home/daniel/wiki.txt\")\n",
    "\n",
    "avg_word_length = text_file.flatMap(lambda line: line.split(\" \"))\n",
    "    \n",
    "def validate_word(word):\n",
    "    if word == \"-\":\n",
    "        return 0\n",
    "    elif word.endswith(\".\") or word.endswith(\",\") or word.endswith(\":\") or word.endswith(\"?\") or word.endswith(\"!\") or word.endswith(\";\"):\n",
    "        return len(word) - 1\n",
    "    else:\n",
    "        return len(word)\n",
    "    \n",
    "lineLengths = avg_word_length.map(lambda s: validate_word(s))\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "print(\"Средняя длина слова:\", totalLength / avg_word_length.count())\n",
    "sc.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Напишите программу, которая находит самое частоупотребляемое слово, состоящее из латинских букв."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самое частоупотребляемое слово: wikipedia\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sc = SparkContext()\n",
    "lines = sc.textFile(\"/home/daniel/wiki.txt\")\n",
    "\n",
    "words = lines.flatMap(lambda line: re.findall(r'\\b(?!ru|http|org)[a-zA-Z]+\\b', line))\n",
    "lowercase_words = words.map(lambda word: word.lower())\n",
    "word_counts = lowercase_words.countByValue()\n",
    "most_frequent_word = max(word_counts, key=word_counts.get)\n",
    "\n",
    "print(\"Самое частоупотребляемое слово:\", most_frequent_word)\n",
    "sc.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Все слова, которые более чем в половине случаев начинаются с большой буквы и встречаются больше 10 раз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "НЭПа: 26\n",
      "СПИДа: 13\n",
      "РСДРП(б),: 22\n",
      "ГУЛАГа: 11\n",
      "UltraSPARC: 20\n",
      "РСДРП(б): 33\n",
      "МВт: 46\n",
      "ВУЗов: 17\n",
      "ЮКОСа: 40\n",
      "BeOS: 26\n",
      "ГГц: 19\n",
      "OpenBSD: 13\n",
      "ЗаПЧЕЛ: 20\n",
      "SNMPv1: 12\n",
      "СвДП: 33\n",
      "МПа: 27\n",
      "HCl: 21\n",
      "МГц: 70\n",
      "FreeBSD: 57\n",
      "СПбГУ: 25\n",
      "ГэВ: 13\n",
      "NetBIOS: 12\n",
      "СПИДом: 11\n",
      "NSFNet: 19\n",
      "NaOH: 14\n",
      "МэВ: 15\n",
      "TeX: 24\n",
      "NetBSD: 14\n",
      "MySQL: 43\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "conf = SparkConf().setAppName(\"task 4.4\")\n",
    "sc = SparkContext(conf=conf)\n",
    "lines = sc.textFile(\"/home/daniel/wiki.txt\")\n",
    "words = lines.flatMap(lambda line: line.split())\n",
    "big_words = words.filter(lambda word: word[0].isupper() and word != word.upper())\n",
    "word_counts = big_words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "frequent_words = word_counts.filter(lambda x: x[1] > 10)\n",
    "finally_words = frequent_words.filter(lambda x: sum(1 for word in x[0] if word.isupper()) / len(x[0]) > 0.5)\n",
    "for word, count in finally_words.collect():\n",
    "    print(\"{}: {}\".format(word, count))\n",
    "sc.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Напишите программу, которая с помощью статистики определяет устойчивые сокращения вида `пр.`, `др.`, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "см.\n",
      "др.\n",
      "ст.\n",
      "км.\n",
      "гг.\n",
      "мм.\n",
      "кг.\n",
      "ок.\n",
      "св.\n",
      "вв.\n",
      "кв.\n",
      "им.\n",
      "пр.\n",
      "га.\n",
      "ул.\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"task 4.5\")\n",
    "sc = SparkContext(conf=conf)\n",
    "lines = sc.textFile(\"/home/daniel/wiki.txt\")\n",
    "words = lines.flatMap(lambda line: line.split())\n",
    "words = lines.flatMap(lambda line: re.findall(r'[а-яА-Я]+\\.', line))\n",
    "lowercase_words = words.filter(lambda word: not word[0].isupper() and len(word) <= 3 and len(word) > 2)\n",
    "finally_words = {}\n",
    "for word in lowercase_words.collect():\n",
    "    if word in finally_words:\n",
    "        finally_words[word] += 1\n",
    "    else:\n",
    "        finally_words[word] = 1\n",
    "\n",
    "n = 200\n",
    "for word in finally_words:\n",
    "    if finally_words[word] > n:\n",
    "        print(word)\n",
    "sc.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Напишите программу, которая с помощью статистики определяет устойчивые сокращения вида  `т.п.`, `н.э.`, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "н.э.\n",
      "т.д.\n",
      "т.п.\n",
      "с.ш.\n",
      "в.д.\n",
      "ю.ш.\n",
      "г.р.\n",
      "г.г.\n",
      "т.е.\n",
      "т.к.\n",
      "т.н.\n",
      "л.с.\n",
      "т.ч.\n",
      "э.д.\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"task 4.5\")\n",
    "sc = SparkContext(conf=conf)\n",
    "lines = sc.textFile(\"/home/daniel/wiki.txt\")\n",
    "words = lines.flatMap(lambda line: line.split())\n",
    "words = lines.flatMap(lambda line: re.findall(r'[а-яА-Я]+\\.+[а-яА-Я]+\\.', line))\n",
    "lowercase_words = words.filter(lambda word: not word[0].isupper() and len(word) <= 4 and len(word) > 2)\n",
    "finally_words = {}\n",
    "for word in lowercase_words.collect():\n",
    "    if word in finally_words:\n",
    "        finally_words[word] += 1\n",
    "    else:\n",
    "        finally_words[word] = 1\n",
    "\n",
    "\n",
    "n = 10\n",
    "for word in finally_words:\n",
    "    if finally_words[word] > n:\n",
    "        print(word)\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
